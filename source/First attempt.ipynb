{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    file_paths = [f for f in glob.glob(\"../data/*.txt\")]\n",
    "    data = {}\n",
    "    for path in file_paths:\n",
    "        file = open(path , \"rb\")\n",
    "        data[os.path.basename(path)] = pickle.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    file_paths = [f for f in glob.glob(\"../data/secret_test_set/*.txt\")]\n",
    "    secret_test_data = {}\n",
    "    for path in file_paths:\n",
    "        file = open(path , \"rb\")\n",
    "        secret_test_data[os.path.basename(path)] = pickle.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    return data, secret_test_data\n",
    "\n",
    "\n",
    "# this is terrifying code, there ought to be a better way to split these lists into 500 word segments\n",
    "def segment_data(input_data_dict):\n",
    "    labels, processed_data = [], []\n",
    "    for file_path, tag_list in input_data_dict.items():\n",
    "        label = get_label_from_file_path(file_path)\n",
    "        string = ''\n",
    "        for i, tag in enumerate(tag_list):\n",
    "            string += tag.lemma + ' '\n",
    "            if i % 500 == 0:\n",
    "                labels.append(label)\n",
    "                processed_data.append(string)\n",
    "                string = ''\n",
    "        labels.append(label)\n",
    "        processed_data.append(string)\n",
    "    return labels, processed_data\n",
    "\n",
    "\n",
    "def get_label_from_file_path(file_path):\n",
    "    label = re.search('(?<=-)(.+)(?=-)', file_path)\n",
    "    if label is None:\n",
    "        return file_path\n",
    "    else:\n",
    "        return label.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "book_data_dict, secret_test_set = load_data()\n",
    "labels, processed_data = segment_data(book_data_dict)\n",
    "secret_test_labels, secret_test_processed_data = segment_data(secret_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "\n",
    "text_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "    #('clf', SGDClassifier(loss='hinge', penalty='l2')) # see https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_data, labels, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(secret_test_set['1892-Zola-LE DEBACLE.txt'][0:100])\n",
    "#print(secret_test_processed_data[0])\n",
    "#print(secret_test_labels)\n",
    "#print(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Andre Breton       0.00      0.00      0.00        12\n",
      "             Andre Gide       0.00      0.00      0.00        62\n",
      "               Bernanos       0.00      0.00      0.00        20\n",
      "                 Celine       0.00      0.00      0.00       201\n",
      "          Chateaubriand       0.00      0.00      0.00        16\n",
      "        Claude Farrère       0.00      0.00      0.00        23\n",
      "                Colette       0.00      0.00      0.00        91\n",
      "       Françoise Sagan       0.00      0.00      0.00        10\n",
      "              Giraudoux       0.00      0.00      0.00        31\n",
      "        Gobineau Joseph       0.00      0.00      0.00        46\n",
      "               Huysmans       0.00      0.00      0.00        43\n",
      "             Jean Giono       0.00      0.00      0.00        14\n",
      "       Jean Paul Sartre       0.00      0.00      0.00        51\n",
      "         Joseph Malegue       0.00      0.00      0.00       146\n",
      "           Jules Renard       0.00      0.00      0.00        21\n",
      "            Jules Verne       0.00      0.00      0.00        93\n",
      "                   Loti       0.00      0.00      0.00        25\n",
      "          Louis Pergaud       0.00      0.00      0.00        26\n",
      "         Martin Du Gard       0.00      0.00      0.00        20\n",
      "             Maupassant       0.22      1.00      0.36       397\n",
      "Maxence Van Der Meersch       0.00      0.00      0.00        91\n",
      "                 Pegard       0.00      0.00      0.00        46\n",
      "                 Proust       0.00      0.00      0.00       154\n",
      "     Simone de Beauvoir       0.00      0.00      0.00        75\n",
      "         Valery Larbaud       0.00      0.00      0.00         8\n",
      "                Vogüé       0.00      0.00      0.00        61\n",
      "                   Zola       0.99      0.89      0.94       386\n",
      "\n",
      "              micro avg       0.34      0.34      0.34      2169\n",
      "              macro avg       0.04      0.07      0.05      2169\n",
      "           weighted avg       0.22      0.34      0.23      2169\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Chateaubriand       0.00      0.00      0.00        27\n",
      "      Colette       0.00      0.00      0.00       269\n",
      "    Giraudoux       0.00      0.00      0.00       115\n",
      "   Jean Giono       0.00      0.00      0.00        96\n",
      "   Maupassant       0.00      0.00      0.00         0\n",
      "      Pergaud       0.00      0.00      0.00        92\n",
      "         Zola       0.99      0.49      0.66       366\n",
      "\n",
      "    micro avg       0.19      0.19      0.19       965\n",
      "    macro avg       0.14      0.07      0.09       965\n",
      " weighted avg       0.38      0.19      0.25       965\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "text_classifier.fit(X_train, y_train)\n",
    "    \n",
    "predicted = text_classifier.predict(X_test)\n",
    "np.mean(predicted == y_test)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "secret_test_predicted = text_classifier.predict(secret_test_processed_data)\n",
    "np.mean(secret_test_predicted == secret_test_labels)\n",
    "print(metrics.classification_report(secret_test_labels, secret_test_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
